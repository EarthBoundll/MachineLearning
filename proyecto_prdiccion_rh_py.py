# -*- coding: utf-8 -*-
"""Proyecto_Prdiccion_RH.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FfgJgxwloi3JWcXtF57g9L8RVuHUDSK1
"""

# =============================================================
# HR Analytics - PredicciÃ³n de Promociones (VersiÃ³n FINAL)
# Autor: Charles
# Contiene:
# - Limpieza, EDA, Encoding, Escalado
# - Undersampling, Oversampling, SMOTE
# - Modelos: Logistic Regression, Random Forest, XGBoost
# - Modelo Deep Learning con TensorFlow (DNN)
# =============================================================

# 1ï¸âƒ£ ImportaciÃ³n de librerÃ­as
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler, SMOTE

from sklearn.utils.class_weight import compute_class_weight

import joblib
import warnings
warnings.filterwarnings('ignore')

# TensorFlow / Keras
import tensorflow as tf
from tensorflow.keras import models, layers, callbacks

# =============================================================
# 2ï¸âƒ£ Carga de datos
# =============================================================
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("HRAnalytics(in).csv")
print("Dimensiones:", df.shape)
display(df.head())

# =============================================================
# 3ï¸âƒ£ Limpieza de datos
# =============================================================
print("\nValores nulos antes:")
print(df.isnull().sum())

for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].median())

print("\nValores nulos despuÃ©s:")
print(df.isnull().sum())

# =============================================================
# 4ï¸âƒ£ EDA BÃ¡sico
# =============================================================
plt.figure(figsize=(6,4))
sns.countplot(x='is_promoted', data=df)
plt.title("DistribuciÃ³n de clases (is_promoted)")
plt.show()

print("\nProporciÃ³n de clases (is_promoted):")
print(df['is_promoted'].value_counts(normalize=True))

# =============================================================
# 5ï¸âƒ£ Encoding de variables categÃ³ricas
# =============================================================
categorical_cols = df.select_dtypes(include=['object']).columns
encoder = LabelEncoder()

for col in categorical_cols:
    df[col] = encoder.fit_transform(df[col])

print("\nColumnas categÃ³ricas codificadas:")
print(list(categorical_cols))

# =============================================================
# 6ï¸âƒ£ SeparaciÃ³n de variables
# =============================================================
X = df.drop(['employee_id', 'is_promoted'], axis=1)
y = df['is_promoted']

print("\nShape X:", X.shape)
print("Shape y:", y.shape)

# =============================================================
# 7ï¸âƒ£ Train/Test Split
# =============================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.20,
    stratify=y,
    random_state=42
)

print("\nDistribuciÃ³n y_train:", np.bincount(y_train))
print("DistribuciÃ³n y_test:", np.bincount(y_test))

# =============================================================
# 8ï¸âƒ£ Escalado
# =============================================================
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s  = scaler.transform(X_test)

# =============================================================
# 9ï¸âƒ£ Estrategias de Balanceo
# =============================================================

# ðŸ”µ 9.1 Undersampling
rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(X_train_s, y_train)

# ðŸ”´ 9.2 Oversampling
ros = RandomOverSampler(random_state=42)
X_ros, y_ros = ros.fit_resample(X_train_s, y_train)

# ðŸŸ£ 9.3 SMOTE
smote = SMOTE(random_state=42, k_neighbors=5)
X_sm, y_sm = smote.fit_resample(X_train_s, y_train)

print("\nDistribuciÃ³n despuÃ©s de balanceo:")
print("ðŸ”µ Under:", np.bincount(y_rus))
print("ðŸ”´ Over:", np.bincount(y_ros))
print("ðŸŸ£ SMOTE:", np.bincount(y_sm))

# =============================================================
# ðŸ”¥ 1ï¸âƒ£0ï¸âƒ£ Entrenamiento de Modelos ClÃ¡sicos con cada estrategia
# =============================================================

def entrenar_modelos(nombre, Xtr, ytr, Xtest, ytest):
    print("\n\n==============================")
    print(f" MODELOS CON: {nombre}")
    print("==============================")

    # Logistic Regression
    lr = LogisticRegression(max_iter=2000)
    lr.fit(Xtr, ytr)
    pred_lr = lr.predict(Xtest)

    # Random Forest
    rf = RandomForestClassifier(n_estimators=200, random_state=42)
    rf.fit(Xtr, ytr)
    pred_rf = rf.predict(Xtest)

    # XGBoost
    xgb_model = xgb.XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42
    )
    xgb_model.fit(Xtr, ytr)
    pred_xgb = xgb_model.predict(Xtest)

    modelos = {
        "LR": (lr, pred_lr),
        "RF": (rf, pred_rf),
        "XGB": (xgb_model, pred_xgb)
    }

    for nombre_modelo, (modelo, pred) in modelos.items():
        print(f"\nðŸ“Œ Modelo: {nombre_modelo} ({nombre})")
        print(classification_report(ytest, pred, digits=4))

    # Devuelvo solo los modelos
    return {
        "LR": lr,
        "RF": rf,
        "XGB": xgb_model
    }

# Entrenar con cada tÃ©cnica:
modelos_original = entrenar_modelos("ORIGINAL", X_train_s, y_train, X_test_s, y_test)
modelos_under    = entrenar_modelos("UNDER",    X_rus,    y_rus,  X_test_s, y_test)
modelos_over     = entrenar_modelos("OVER",     X_ros,    y_ros,  X_test_s, y_test)
modelos_smote    = entrenar_modelos("SMOTE",    X_sm,     y_sm,   X_test_s, y_test)

# =============================================================
# 11ï¸âƒ£ SelecciÃ³n del MEJOR modelo clÃ¡sico en funciÃ³n del Recall de la clase 1
# =============================================================
def mejor_modelo(model_dict, nombre, Xtest, ytest):
    resultados = []
    for etiqueta, modelo in model_dict.items():
        pred = modelo.predict(Xtest)
        report = classification_report(ytest, pred, output_dict=True)
        recall_1 = report["1"]["recall"]
        precision_1 = report["1"]["precision"]
        f1_1 = report["1"]["f1-score"]
        resultados.append((etiqueta, recall_1, precision_1, f1_1))

    best = max(resultados, key=lambda x: x[1])  # max por recall
    print(f"\nâ­ Mejor modelo ({nombre}) segÃºn RECALL clase 1:")
    print(f"   Modelo: {best[0]} | Recall: {best[1]:.4f} | Precision: {best[2]:.4f} | F1: {best[3]:.4f}")
    return best

mejor_modelo_original = mejor_modelo(modelos_original, "ORIGINAL", X_test_s, y_test)
mejor_modelo_under    = mejor_modelo(modelos_under,    "UNDER",    X_test_s, y_test)
mejor_modelo_over     = mejor_modelo(modelos_over,     "OVER",     X_test_s, y_test)
mejor_modelo_smote    = mejor_modelo(modelos_smote,    "SMOTE",    X_test_s, y_test)

# =============================================================
# 1ï¸âƒ£2ï¸âƒ£ Modelo Deep Learning con TensorFlow (usando SMOTE)
# =============================================================

print("\n\n==============================")
print(" MODELO DEEP LEARNING (TensorFlow + SMOTE)")
print("==============================")

# 1ï¸âƒ£ Dataset para Deep Learning: usamos el dataset SMOTE ya escalado
X_train_dl = X_sm
y_train_dl = y_sm.values if isinstance(y_sm, pd.Series) else y_sm

X_test_dl = X_test_s
y_test_dl = y_test.values if isinstance(y_test, pd.Series) else y_test

# 2ï¸âƒ£ CÃ¡lculo de pesos de clase
class_weights_array = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train_dl),
    y=y_train_dl
)
class_weights = {i: class_weights_array[i] for i in range(len(class_weights_array))}
print("Class Weights:", class_weights)

# 3ï¸âƒ£ DefiniciÃ³n del modelo DNN
model_tf = models.Sequential([
    layers.Dense(64, activation="relu", input_shape=(X_train_dl.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(32, activation="relu"),
    layers.Dropout(0.2),
    layers.Dense(1, activation="sigmoid")
])

model_tf.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=[
        "accuracy",
        tf.keras.metrics.Precision(name="precision"),
        tf.keras.metrics.Recall(name="recall")
    ]
)

model_tf.summary()

# 4ï¸âƒ£ Early Stopping
es = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)

# 5ï¸âƒ£ Entrenamiento
history = model_tf.fit(
    X_train_dl,
    y_train_dl,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    class_weight=class_weights,
    callbacks=[es],
    verbose=1
)

# 6ï¸âƒ£ EvaluaciÃ³n en test
print("\nðŸ” EvaluaciÃ³n en conjunto de TEST (TensorFlow):")
results = model_tf.evaluate(X_test_dl, y_test_dl, verbose=0)
print(f"Loss: {results[0]:.4f} | Accuracy: {results[1]:.4f} | Precision: {results[2]:.4f} | Recall: {results[3]:.4f}")

# Reporte detallado
y_pred_proba_tf = model_tf.predict(X_test_dl).ravel()
y_pred_tf = (y_pred_proba_tf >= 0.5).astype(int)

print("\nðŸ§¾ Classification Report (TensorFlow):")
print(classification_report(y_test_dl, y_pred_tf, digits=4))

print("\nðŸ§± Matriz de confusiÃ³n (TensorFlow):")
print(confusion_matrix(y_test_dl, y_pred_tf))

# =============================================================
# 1ï¸âƒ£3ï¸âƒ£ Guardar modelos finales
# =============================================================

# Modelo clÃ¡sico recomendado: XGB con SMOTE (por ejemplo)
modelo_clasico_final = modelos_smote["XGB"]
joblib.dump(modelo_clasico_final, "modelo_xgb_smote_promociones.pkl")

# Guardar scaler
joblib.dump(scaler, "scaler_promociones.pkl")

# Guardar modelo TensorFlow
model_tf.save("modelo_tensorflow_promociones.h5")

print("\nâœ… Modelos guardados como:")
print("   - modelo_xgb_smote_promociones.pkl")
print("   - scaler_promociones.pkl")
print("   - modelo_tensorflow_promociones.h5")

# =============================================================
# 1ï¸âƒ£4ï¸âƒ£ Guardar predicciones finales (TensorFlow)
# =============================================================
df_pred = df.copy()
X_full_scaled = scaler.transform(X)
y_full_pred_proba_tf = model_tf.predict(X_full_scaled).ravel()
y_full_pred_tf = (y_full_pred_proba_tf >= 0.5).astype(int)

df_pred["prediccion_tensorflow"] = y_full_pred_tf
df_pred.to_csv("predicciones_promociones_tensorflow.csv", index=False)

print("\nðŸ“‚ Archivo 'predicciones_promociones_tensorflow.csv' generado con Ã©xito.")